{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitted by:\n",
    " \n",
    "* Name 1: Ido Hayun\n",
    "* Name 2: Lidor Tevet\n",
    "* Name 3: Tim Buchbinder\n",
    "## To be submitted by: Unknown yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paint_the_circle Project\n",
    "\n",
    "This project identifies and paints circles given in RGB images. Specifically, each input image contains a rectangle, a triangle and a circle. The image is fed into a neural netork, whose outputs are the center and radius of the circle within the image. The domain considered is \"[0,1]x[0,1] \"box\". PyTorch is the platform used. The goal is to paint the circle.\n",
    "\n",
    "### **You are given the following files:**\n",
    "  Three directories named \"train\", \"validation\", and \"test\" each containing the following:\n",
    "  1. a directory called \"images\" with RGB png files of size 128x128 (so image that is read has shape (3,128,128))\n",
    "  2. a file \"labels.txt\" containing the center points (as x,y) and radius of the circles for all images in \"images\" directory\n",
    "\n",
    "Since this problem is somewhat open ended, you are given a suggested structure and some functionality that is already implemented for you (see below). You are not at all obigated to follow this structure or use the code given. \n",
    "\n",
    "**Note:** \n",
    "- \"network\" and \"model\" are used interchangeably throughout this documentation.\n",
    " \n",
    " \n",
    "### **General structure (in order):**\n",
    " 1. DataSet and DataLoader            - this is how you load data into the network **(given to you)**\n",
    " 2. Neural Network definition         - the class that defines your model   **(need to implement)** //from the file (first try)\n",
    " 3. calculate model parameters number - useful to get a feel for network's size **(given to you)**\n",
    " 4. loss function definition          - to be used in training **(need to implement)* \n",
    " 5. create an optimizer               - choose your optimizer **(need to implement)** start with adam\n",
    " 6. estimate number of ops per forward feed - the bigger this is the slower the training **(given to you)**\n",
    " 7. estimate ops per forward feed     - use function given to you to estimate this **(implement for convenience)** // ido\n",
    " 8. view images and target and net labels - example code to help you see how to use loaders **(given to you)**\n",
    " 9. validate_model                   - returns avg loss per image for a model and loader  **(need to implement)**\n",
    " 10. train_model                      - trains the network **(need to implement)**\n",
    " 11. plot train/validaion losses      - visualizing train and validation losses from training **(given to you)**\n",
    " 12. save/load model                  - Allows you to save and load model to disk for later use **(given to you)**\n",
    " 13. visualizing images    - Painting circles prodiced by network on images from a given loader **(given to you)**\n",
    " \n",
    " \n",
    "### ** What you need to do **\n",
    "You have 5 (and a half...) things to do:\n",
    " 1. Create a CircleNet class, which is your network model. This is a key component. The output of your model should be 3 numbers that represent the the center and radius of the circle in the input image fed into your network. You should keep in mind the number of parameters in your model. If there are too many, it may overfit (and take longer to run). If there are too few, you it may not be able to learn the task needed. A typical structure would have convolutional layers first and fully connected at the end, thus reducing number of parameteres. Consider which activation function you want to use, and whether or not you wish to use batch normalization or dropout. Pooling layers are also possible. Be creative.\n",
    " \n",
    " 2. Create a loss function. This is another key component as it defines what it means for two circles (the true circle and its estimation) to be similar or not. Namely, two circles (represented by two centers and two radii: ground truth (\"labels\") and network outputs (\"outputs\")), whose images look similar should have a smaller loss than two circles whose images look less simialr. Think about how you would quantify \"closeness\"/\"similarity\" of circles.\n",
    "\n",
    " 3. Choose an optimizer. Look here for some ideas: https://pytorch.org/docs/master/optim.html\n",
    " \n",
    " 4. (This is the \"half\" thing to do). For your conveinece you may want to use the function calc_ops() that is given to you to calculate an estimate of the number of operations that your network does per feed forward. To do this, you need to enter your own network structure. An example of an arbitrary network is privided to you.\n",
    " \n",
    " 5. Create a validate_model function that assesses (tests) the performance of a model. It returns the average loss per image for a given loader. It can be run on any set of data (train, validation, or test). You may want to run this function on validation set (loader) from within the train function (see below) train after each epoch so as to see how loss behaves on validation during the training process.\n",
    " \n",
    " 6. Create a train_model function that trains model. This function updates the following parameteres: model, train_losses, and validation_losses. Model is updated simply by the training processes when optimzer.step() is called. The other two parameteres are lists that hold the average loss per image for the corresponding data (train or validation). After every epoch (i.e., iteration that goes over the entire train data), you should save to these lists the average loss per image for the corresponding data loader. These lists are useful in that you can plot them (functionality given to you) and observe how model behaves. Observe: this fucnton returs nothing, however, it updates parameteres by reference. Specifically, model is updated (trained), and so are train and validation losses values.\n",
    " \n",
    "**Note:** You are given three sets of data: trian, validation, and test. It is recommended that test not be touched until the very end, and validation be used to get a sense of your network's performance. \n",
    "\n",
    "### ** Running on a GPU: **\n",
    "It is not necessary to use a GPU for this project. However, if you choose to do so, you will gain a major speedup to your training, which will save you much time. The code given to you identifies the hardware used and will  automatically run on either a GPU or CPU. \n",
    "\n",
    "### ** Useful links: **\n",
    "1. PyTorch master tutorial - VERY useful: https://pytorch.org/docs/master/nn.html\n",
    "2. PyTorch optimizers: https://pytorch.org/docs/master/optim.html\n",
    "3. A list of possible reasons why things go wrong: https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#74de\n",
    "\n",
    "### ** Final tips: **\n",
    "Use the Internet! Things will not work first time. You will get strange error messages. Google them up. The web is  great resource for tackling problems ranging from python error messages, to things not doing what you'd like them to do.\n",
    "\n",
    "\n",
    "### ** Submission Instructions**\n",
    "The project is to be submittd in teams as in the homework. You need to submit the following three files:\n",
    "1. model.dat                            - This is your saved model \n",
    "2. paint_that_circle.ipynb              - This is this notebook containing all of your work\n",
    "3. model.py                             - A file containing your CircleNet class only. \n",
    "\n",
    "Before you submit, run \"check_before_submission.ipynb\" to make sure your model could be properly tested. See instructions for running this notebook inside.\n",
    "\n",
    "Make sure your names appear at the top of this notebook in the appropriate place.\n",
    "\n",
    "### **GOOD LUCK!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "# Device configuration\n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class definition (given to you) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_dir):\n",
    "        \"\"\"\n",
    "        Initializing dataset by generating a dicitonary of labels, where an image file name is the key \n",
    "        and its labels are the contents of that entry in the dictionary. Images are not loaded. This way it\n",
    "        is possible to iterate over arbitrarily large datasets (limited by labels dicitonary fitting \n",
    "        in memory, which is not a problem in practice)\n",
    "        \n",
    "        Args:\n",
    "            dataset_dir : path to directory with images and labels. In this directory we expect to find\n",
    "                          a directory called \"images\" containing the input images, and a file called \n",
    "                          \"labels.txt\" containing desired labels (coefficients)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.labels_dict = self.gen_labels_dict()\n",
    "        self.images_keys = list(self.labels_dict)  # getting the keys of the dictionary as list\n",
    "        self.images_keys.sort()                    # sorting so as to have in alphabetical order \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_dict)\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        \"\"\"\n",
    "        This funtion makes it possible to iterate over the ShapesDataset\n",
    "        Args:\n",
    "            index: running index of images\n",
    "            \n",
    "        Returns:\n",
    "            sample: a dicitionary with three entries:\n",
    "                    1. 'image'  contains the image\n",
    "                    2. 'labels' contains labels (coeffs) corresponding to image\n",
    "                    3. 'fname'  contains name of file (image_key) - may be useful for debugging\n",
    "        \"\"\"\n",
    "        image_key = self.images_keys[index]     # recall - key is the file name of the corresponding image\n",
    "        image = np.array(Image.open(image_key)) # image has shape: (128, 128, 3)\n",
    "        image = image/255.0                     # simple normalization - just to maintain small numbers\n",
    "        image = np.transpose(image, (2, 0, 1))  # network needs RGB channels to be first index\n",
    "        labels = self.labels_dict[image_key]\n",
    "        sample = {'image': image, 'labels': labels, 'fname':image_key}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def gen_labels_dict(self):\n",
    "        \"\"\"\n",
    "        This fucntion generates a dictionary of labels\n",
    "        \n",
    "        Returns:\n",
    "            labels_dict: the key is image file name and the value is the corresponding \n",
    "            array of labels  \n",
    "        \"\"\"\n",
    "        \n",
    "        labels_fname = self.dataset_dir + \"/labels.txt\"\n",
    "        labels_dict = {}\n",
    "        with open(labels_fname, \"r\") as inp:\n",
    "            for line in inp:\n",
    "                line = line.split('\\n')[0]                                      # remove '\\n' from end of line \n",
    "                line = line.split(',')\n",
    "                key  = self.dataset_dir + '/images/' + line[0].strip() + \".png\" # image file name is the key\n",
    "                del line[0]\n",
    "                \n",
    "                list_from_line = [float(item) for item in line]\n",
    "                labels_dict[key] = np.asarray(list_from_line, dtype=np.float32)\n",
    "                        \n",
    "        return labels_dict             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loaders (given to you) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader examples     : 1000\n",
      "validation loader examples: 100\n",
      "test loader examples      : 100\n"
     ]
    }
   ],
   "source": [
    "train_dir      = \"./train/\"\n",
    "validation_dir = \"./validation/\"\n",
    "test_dir       = \"./test/\"\n",
    "\n",
    "\n",
    "train_dataset = ShapesDataset(train_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32,\n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "validation_dataset = ShapesDataset(validation_dir)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                               batch_size=1,\n",
    "                               shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = ShapesDataset(test_dir)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=1,\n",
    "                          shuffle=False)\n",
    "\n",
    "\n",
    "print(\"train loader examples     :\", len(train_dataset)) \n",
    "print(\"validation loader examples:\", len(validation_dataset))\n",
    "print(\"test loader examples      :\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleNet(nn.Module):    # nn.Module is parent class  \n",
    "    def __init__(self):\n",
    "        super(CircleNet, self).__init__()  #calls init of parent class\n",
    "                \n",
    "\n",
    "        #----------------------------------------------\n",
    "        self.conv1_1 = nn.Conv2d(3, 16, kernel_size=3)\n",
    "        self.conv1_2 = nn.Conv2d(16, 16, kernel_size=3)\n",
    "        #self.conv1_2_drop = nn.Dropout2d()\n",
    "        self.conv2_1 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        #self.conv2_2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(29*29*32, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        #----------------------------------------------   \n",
    "    \n",
    "                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed forward through network\n",
    "        Args:\n",
    "            x - input to the network\n",
    "            \n",
    "        Returns \"out\", which is the network's output\n",
    "        \"\"\"\n",
    "        \n",
    "        #----------------------------------------------\n",
    "\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        #x = nn.BatchNorm2d(x) # not sure about the variables\n",
    "        x = F.relu(self.conv1_2(x))#self.conv1_2_drop(self.conv1_2(x)))\n",
    "        #x = nn.BatchNorm2d(x) # not sure about the variables\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        #x = nn.BatchNorm2d(x) # not sure about the variables\n",
    "        x = F.relu(self.conv2_2(x))#self.conv2_2_drop(self.conv2_2(x)))\n",
    "        #x = nn.BatchNorm2d(x) # not sure about the variables\n",
    "        x = F.max_pool2d(x,2)\n",
    "       \n",
    "        x = x.view(-1, 29*29*32)  #not sure about the number      # Flattening tensor. 320 is the volume, and -1 represents batch size \n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x) \n",
    "        x = F.relu(x)\n",
    "        out = x\n",
    "        # cant understand whether out is x or my_loss(x)\n",
    "        #----------------------------------------------\n",
    "\n",
    "       \n",
    "        return out                \n",
    "             \n",
    "             \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(outputs, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "        Args:\n",
    "        outputs - output of network ([batch size, 3]) \n",
    "        labels  - desired labels  ([batch size, 3])\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "    loss = loss.to(device)\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    # implementation needed here \n",
    "    \"\"\"\n",
    "    batchsize = outputs.shape[0]\n",
    "    for i in range(batchsize):\n",
    "        x1,y1,r1 = outputs[i]\n",
    "        x2,y2,r2 = labels[i]\n",
    "        distanst = math.sqrt(((x1 - x2)**2) + ((y1 - y2)**2))\n",
    "        intarea = intersection_area(distanst, r1, r2)\n",
    "        area1 = math.pi * r1**2\n",
    "        area2 = math.pi * r2**2\n",
    "        localloss = -math.log(((intarea/area1) + (intarea/area2) + (1/10000)) / 2, math.e)\n",
    "        if intarea == 0:\n",
    "            loss = loss + (distanst - (r1 + r2))**4\n",
    "        loss = loss + localloss\n",
    "                   \n",
    "    loss = loss / batchsize\n",
    "    \"\"\"\n",
    "    batchsize = outputs.shape[0]\n",
    "    for i in range(batchsize):\n",
    "        x1,y1,r1 = outputs[i]\n",
    "        x2,y2,r2 = labels[i]\n",
    "        loss = loss + (((x1 - x2)**2) + ((y1 - y2)**2))**(0.5) * 2 + abs(r1 - r2)\n",
    "                \n",
    "    loss = loss / batchsize\n",
    "    \n",
    "    # לחשב את השטח המשותף של שני המעגלים \n",
    "    # לקחת את השטח המשותף חלקי השטח של המטרה \n",
    "    # להוסיף לו את השטח המשותף חלקי התוצאה\n",
    "    # לחלק ב 2\n",
    "    # לתוצאה - LOG  \n",
    "    #----------------------------------------------\n",
    "\n",
    "    # Observe: If you need to iterate and add certain values to loss defined above\n",
    "    # you cannot write: loss +=... because this will raise the error: \n",
    "    # \"Leaf variable was used in an inplace operation\"\n",
    "    # Instead, to avoid this error write: loss = loss + ...  \n",
    "    \n",
    "                                      \n",
    "    return loss\n",
    "\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "def intersection_area(d, R, r):\n",
    "    \"\"\"Return the area of intersection of two circles.\n",
    "\n",
    "    The circles have radii R and r, and their centres are separated by d.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if d <= abs(R-r):\n",
    "        # One circle is entirely enclosed in the other.\n",
    "        return np.pi * min(R, r)**2\n",
    "    if d >= r + R:\n",
    "        # The circles don't overlap at all.\n",
    "        return 0\n",
    "\n",
    "    r2, R2, d2 = r**2, R**2, d**2\n",
    "    alpha = math.acos((d2 + r2 - R2) / (2*d*r))\n",
    "    beta = math.acos((d2 + R2 - r2) / (2*d*R))\n",
    "    return ( r2 * alpha + R2 * beta -\n",
    "             0.5 * (r2 * math.sin(2*alpha) + R2 * math.sin(2*beta))\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get number of trainable parameters (given to you)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_params_num(model):\n",
    "    \"\"\"\n",
    "    This fucntion returns the number of trainable parameters of neural network model\n",
    "    You may want to call it after you create your model to see how many parameteres the model has\n",
    "    Args:\n",
    "        model - neural net to examine\n",
    "    \"\"\"\n",
    "    \n",
    "    #filter given iterable with a function that tests each element in the iterable to be true or not\n",
    "    model_parameters = filter(lambda p: p.requires_grad == True, model.parameters()) \n",
    "    params_num = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and choice of optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model trainable parameters: 2708999\n"
     ]
    }
   ],
   "source": [
    "model = CircleNet().to(device)\n",
    "print (\"Number of model trainable parameters:\", get_train_params_num(model))\n",
    "\n",
    "#----------------------------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters()) #, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) # not sure what suppose to be instead of params\n",
    "#----------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an estimate number of operations (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ops(inp_size, net_struct):\n",
    "    \"\"\"\n",
    "    Calculates a rough number of operations for a given network topology\n",
    "    Args:\n",
    "        inp_size - (W,H) of input \n",
    "        net_struct - list of tuples describing structure of network. \n",
    "        \n",
    "        Example:\n",
    "         (('conv2d', (3, 8, 3, 1, 0)),  # cin, cout, kernel, stride, pad\n",
    "          ('conv2d': 83, 8, 3, 1, 0)),\n",
    "          ('MaxPool2d', (2,2)),         # kernel, stride\n",
    "          ('fc': (64, 8)),      \n",
    "          ('fc': (8, 4)))\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    ops = 0\n",
    "    W, H = inp_size\n",
    "    for curr_item in net_struct:\n",
    "        if curr_item[0] == 'conv2d':\n",
    "            cin = curr_item[1][0]\n",
    "            cout = curr_item[1][1]\n",
    "            kernel = curr_item[1][2]\n",
    "            stride = curr_item[1][3]\n",
    "            pad = curr_item[1][4]\n",
    "            W = (W +2*pad - kernel)/stride + 1\n",
    "            H = (H +2*pad - kernel)/stride + 1\n",
    "            curr_ops = (W*H*cin*cout*kernel*kernel)/stride\n",
    "            ops += curr_ops\n",
    "            print (curr_item, \":\",  \"{:,}\".format(int(curr_ops)))\n",
    "        elif curr_item[0] == 'MaxPool2d':\n",
    "            kernel = curr_item[1][0]\n",
    "            stride = curr_item[1][1]\n",
    "            W = (W - kernel)/stride + 1\n",
    "            H = (H - kernel)/stride + 1\n",
    "        else:\n",
    "            curr_ops = curr_item[1][0] * curr_item[1][1]\n",
    "            ops += curr_ops\n",
    "            print (curr_item, \":\",  \"{:,}\".format(int(curr_ops)))\n",
    "            \n",
    "    return int(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check rough number of ops for network (for your convenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv2d', (3, 16, 3, 1, 0)) : 6,858,432\n",
      "('conv2d', (16, 16, 3, 1, 0)) : 35,426,304\n",
      "('conv2d', (16, 32, 3, 1, 0)) : 16,588,800\n",
      "('conv2d', (32, 32, 3, 1, 0)) : 31,002,624\n",
      "('fc', (26912, 100)) : 2,691,200\n",
      "('fc', (100, 10)) : 1,000\n",
      "('fc', (10, 3)) : 30\n",
      "\n",
      "Total ops: 92,568,390\n"
     ]
    }
   ],
   "source": [
    "inp_size = (128,128)\n",
    "\n",
    "# place your network ropology in example_net below to obtain an estimated number of operations for your network\n",
    "example_net = (('conv2d', (3, 16, 3, 1, 0)),\n",
    "               ('conv2d', (16, 16, 3, 1, 0)),\n",
    "               ('MaxPool2d', (2,2)),\n",
    "               ('conv2d', (16, 32, 3, 1, 0)),\n",
    "               ('conv2d', (32, 32, 3, 1, 0)),\n",
    "               ('MaxPool2d', (2,2)),\n",
    "               ('fc', (29*29*32, 100)),\n",
    "               ('fc', (100, 10)),\n",
    "               ('fc', (10, 3)))\n",
    "### my code\n",
    "\n",
    "              \n",
    "ops = calc_ops(inp_size, example_net)\n",
    "print()\n",
    "print(\"Total ops: {:,}\".format(ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View images, target circle labels and  network outputs (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing image:  ./train//images/0068.png\n",
      "Target labels : [0.28, 0.19, 0.18]\n",
      "\n",
      "showing image:  ./train//images/0784.png\n",
      "Target labels : [0.42, 0.75, 0.25]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "View first image of a given number of batches assuming that model has been created. \n",
    "Currently, lines assuming model has been creatd, are commented out. Without a model, \n",
    "you can view target labels and the corresponding images.\n",
    "This is given to you so that you may see how loaders and model can be used. \n",
    "\"\"\"\n",
    "\n",
    "loader = train_loader # choose from which loader to show images\n",
    "bacthes_to_show = 2\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(loader, 0): #0 means that counting starts at zero\n",
    "        inputs = (data['image']).to(device)   # has shape (batch_size, 3, 128, 128)\n",
    "        labels = (data['labels']).to(device)  # has shape (batch_size, 3)\n",
    "        img_fnames = data['fname']            # list of length batch_size\n",
    "        \n",
    "        #outputs = model(inputs.float())\n",
    "        img = Image.open(img_fnames[0])\n",
    "        \n",
    "        print (\"showing image: \", img_fnames[0])\n",
    "        \n",
    "        labels_str = [ float((\"{0:.2f}\".format(x))) for x in labels[0]]#labels_np_arr]\n",
    "        \n",
    "        #outputs_np_arr = outputs[0] # using \".numpy()\" to convert tensor to numpy array\n",
    "        #outputs_str = [ float((\"{0:.2f}\".format(x))) for x in outputs_np_arr]\n",
    "        print(\"Target labels :\", labels_str )\n",
    "        #print(\"network coeffs:\", outputs_str)\n",
    "        print()\n",
    "        #img.show()\n",
    "        \n",
    "        if (i+1) == bacthes_to_show:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, loader):\n",
    "    \"\"\"\n",
    "    This function parses a given loader and returns the avergae (per image) loss \n",
    "    (as defined by \"my_loss\") of the entire dataset associated with the given loader.\n",
    "    \n",
    "    Args:\n",
    "        model  - neural network to examine\n",
    "        loader - where input data comes from (train, validation, or test)\n",
    "        \n",
    "    returns:\n",
    "        average loss per image in variable named \"avg_loss\"\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "                  # (dropout is set to zero)\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # implementation needed here\n",
    "    avg_loss = 0 \n",
    "    model.float()\n",
    "    with torch.no_grad():\n",
    "        correct = 0;\n",
    "        for batch_idx, data in enumerate(loader):\n",
    "                data, target = data['image'], data['labels']\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                #optimizer.zero_grad()\n",
    "                output = model(data.float())\n",
    "                loss = my_loss(output, target).item()\n",
    "                avg_loss = avg_loss + loss\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = correct + pred.eq(target.view_as(pred)).sum().item()\n",
    "        avg_loss = avg_loss / len(loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testset_loader.dataset),\n",
    "        100. * correct / len(testset_loader.dataset)))\n",
    "    #----------------------------------------------\n",
    "    \n",
    "\n",
    "    model.train()  #back to default\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                optimizer,\n",
    "                train_loader,\n",
    "                validation_loader,\n",
    "                train_losses,\n",
    "                validation_losses,\n",
    "                epochs=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains a neural network. \n",
    "    Args:\n",
    "        model               - model to be trained\n",
    "        optimizer           - optimizer used for training\n",
    "        train_loader        - loader from which data for training comes \n",
    "        validation_loader   - loader from which data for validation comes (maybe at the end, you use test_loader)\n",
    "        train_losses        - adding train loss value to this list for future analysis\n",
    "        validation_losses   - adding validation loss value to this list for future analysis\n",
    "        epochs              - number of runs over the entire data set \n",
    "    \"\"\"\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    # implementation needed here\n",
    "\n",
    "    model.train()  # set training mode  ????\n",
    "    model.float()\n",
    "    for ep in range(epochs):\n",
    "        test_loss = 0\n",
    "        for batch_idx, data in enumerate(train_loader): # enumerate ?\n",
    "            data, target = data['image'], data['labels']\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.float())\n",
    "            tr_loss = my_loss(output, target)\n",
    "            tr_loss.backward()\n",
    "            optimizer.step()\n",
    "            test_loss = test_loss + tr_loss\n",
    "        train_losses.append(test_loss / len(train_loader.dataset))\n",
    "        validation_result = validate_model(model,validation_loader)\n",
    "        validation_losses.append(validation_result)  \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            ep, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), tr_loss.item()))\n",
    "    #----------------------------------------------\n",
    "    return \n",
    " \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual train (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using two lists (train_losses, validation_losses) containing history of losses \n",
    "# (i.e., loss for each training epoch) for train and validation sets. \n",
    "# If thess are not defined, we define them. Otherwise, the function train_model\n",
    "# updates these two lists (by adding loss values when it is called for further training) \n",
    "# in order to be able to visualize train and validation losses\n",
    "\n",
    "\n",
    "if not 'train_losses' in vars():\n",
    "    train_losses = []\n",
    "if not 'validation_losses' in vars():\n",
    "    validation_losses = []\n",
    "\n",
    "\n",
    "train_model(model, \n",
    "            optimizer,\n",
    "            train_loader, \n",
    "            validation_loader, \n",
    "            train_losses, \n",
    "            validation_losses,\n",
    "            epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot losses from training process (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iteration = np.arange(0., len(train_losses))\n",
    "print(len(validation_losses))\n",
    "plt.plot(iteration, train_losses, 'g-', iteration, validation_losses, 'r-')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load Model (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, train_losses, validation_losses, save_dir):\n",
    "    \"\"\"\n",
    "    saving model, train losses, and validation losses\n",
    "    Args:\n",
    "        model              - NN to be saved\n",
    "        train_losses       - history of losses for training dataset\n",
    "        validation_losses  - history of losses for validation dataset\n",
    "        save_dir           - directory where to save the above\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    files = glob.glob(save_dir + '*')\n",
    "    for f in files:\n",
    "        os.remove(f) \n",
    "    \n",
    "    \n",
    "    torch.save(model, save_dir + \"/model.dat\")\n",
    "    \n",
    "    train_losses_f = open(save_dir + \"/train_losses.txt\", \"wt\")\n",
    "    train_losses_f.writelines( \"%.3f\\n\" % item for item in train_losses)\n",
    "    \n",
    "    validation_losses_f = open(save_dir + \"/validation_losses.txt\", \"wt\")\n",
    "    validation_losses_f.writelines( \"%.3f\\n\" % item for item in validation_losses)\n",
    "\n",
    "    return\n",
    "   \n",
    "\n",
    "def load(save_dir):\n",
    "    \"\"\"\n",
    "    loading model, train losses, and validation losses\n",
    "    Args:\n",
    "       save_dir  - dir name from where to load \n",
    "    \"\"\"\n",
    "    \n",
    "    model = torch.load(save_dir + \"/model.dat\") \n",
    "    \n",
    "    train_losses_f = open(save_dir + \"/train_losses.txt\", \"rt\")\n",
    "    train_losses   = train_losses_f.readlines()\n",
    "    train_losses   = [float(num) for num in train_losses]\n",
    "    \n",
    "    validation_losses_f = open(save_dir + \"/validation_losses.txt\", \"rt\")\n",
    "    validation_losses   = validation_losses_f.readlines()\n",
    "    validation_losses   = [float(num) for num in validation_losses]\n",
    "    \n",
    "    return (model, train_losses, validation_losses)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CircleNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "D:\\ProgramData\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Create a directory, for example \"./saves_12/\", where you place your saved models\n",
    "\n",
    "save(model, train_losses, validation_losses, \"./saves_12/\")\n",
    "\n",
    "model, train_losses, validation_losses = load(\"./saves_12/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paint circles on loader images (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paint_loader_circles(model, loader, out_dir):\n",
    "    \"\"\"\n",
    "    This fucntion receives a model, a loader and an output directory. \n",
    "    For each image in the loader it paints a circle that the model identifies. \n",
    "    The images are saved in the given out_dir diretory. \n",
    "    Args:\n",
    "        model   - network for idneitfying circles\n",
    "        loader  - input data to use \n",
    "        out_dir - ouptut directory name (e.g.: 'draws/'). If directory does not exist, it is created.\n",
    "                  If it exists, its files are deleted.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "                  # (dropout is set to zero)\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    files = glob.glob(out_dir + '*')\n",
    "    for f in files:\n",
    "        os.remove(f) \n",
    "  \n",
    "        \n",
    "    for data in loader:\n",
    "        # get inputs\n",
    "        inputs = (data['image']).to(device)\n",
    "        labels = (data['labels']).to(device)  # not using \n",
    "        img_fnames = data['fname'] \n",
    "      \n",
    "        \n",
    "        # forward\n",
    "        outputs = model(inputs.float())\n",
    "        curr_batch_size = np.shape(outputs)[0]\n",
    "        image_size = np.shape(inputs[0])  # image_size = [3, w, h]\n",
    "        _, width, height = image_size\n",
    "        assert (width == height)\n",
    "        \n",
    "        for i in range (curr_batch_size): \n",
    "            x0 = (outputs[i, 0].item()) * width\n",
    "            y0 = (1-outputs[i, 1].item()) * height\n",
    "            r  = outputs[i, 2].item() * width #assume width=height here. Otherwise, circle becomes ellipse\n",
    "   \n",
    "            fname = img_fnames[i]\n",
    "            k+=1\n",
    "            print (str(k) + \".   \" + fname)\n",
    "\n",
    "            img = Image.open(fname)\n",
    "            draw = ImageDraw.Draw(img, 'RGBA')\n",
    "    \n",
    "            draw.ellipse((x0 - r, y0 - r, x0 + r ,y0 + r), fill=(160, 64, 0, 90), outline=None)\n",
    "    \n",
    "            img.save(out_dir + fname.split('/')[-1])\n",
    "    \n",
    "        \n",
    "    model.train()  #back to default\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example of how to paint circles produced by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "1.   ./validation//images/0000.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "2.   ./validation//images/0001.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "3.   ./validation//images/0002.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "4.   ./validation//images/0003.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "5.   ./validation//images/0004.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "6.   ./validation//images/0005.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "7.   ./validation//images/0006.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "8.   ./validation//images/0007.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "9.   ./validation//images/0008.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "10.   ./validation//images/0009.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "11.   ./validation//images/0010.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "12.   ./validation//images/0011.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "13.   ./validation//images/0012.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "14.   ./validation//images/0013.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "15.   ./validation//images/0014.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "16.   ./validation//images/0015.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "17.   ./validation//images/0016.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "18.   ./validation//images/0017.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "19.   ./validation//images/0018.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "20.   ./validation//images/0019.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "21.   ./validation//images/0020.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "22.   ./validation//images/0021.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "23.   ./validation//images/0022.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "24.   ./validation//images/0023.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "25.   ./validation//images/0024.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "26.   ./validation//images/0025.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "27.   ./validation//images/0026.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "28.   ./validation//images/0027.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "29.   ./validation//images/0028.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "30.   ./validation//images/0029.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "31.   ./validation//images/0030.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "32.   ./validation//images/0031.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "33.   ./validation//images/0032.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "34.   ./validation//images/0033.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "35.   ./validation//images/0034.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "36.   ./validation//images/0035.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "37.   ./validation//images/0036.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "38.   ./validation//images/0037.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "39.   ./validation//images/0038.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "40.   ./validation//images/0039.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "41.   ./validation//images/0040.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "42.   ./validation//images/0041.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "43.   ./validation//images/0042.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "44.   ./validation//images/0043.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "45.   ./validation//images/0044.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "46.   ./validation//images/0045.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "47.   ./validation//images/0046.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "48.   ./validation//images/0047.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "49.   ./validation//images/0048.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "50.   ./validation//images/0049.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "51.   ./validation//images/0050.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "52.   ./validation//images/0051.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "53.   ./validation//images/0052.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "54.   ./validation//images/0053.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "55.   ./validation//images/0054.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "56.   ./validation//images/0055.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "57.   ./validation//images/0056.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "58.   ./validation//images/0057.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "59.   ./validation//images/0058.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "60.   ./validation//images/0059.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "61.   ./validation//images/0060.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "62.   ./validation//images/0061.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "63.   ./validation//images/0062.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "64.   ./validation//images/0063.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "65.   ./validation//images/0064.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "66.   ./validation//images/0065.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "67.   ./validation//images/0066.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "68.   ./validation//images/0067.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "69.   ./validation//images/0068.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "70.   ./validation//images/0069.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "71.   ./validation//images/0070.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "72.   ./validation//images/0071.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "73.   ./validation//images/0072.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "74.   ./validation//images/0073.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "75.   ./validation//images/0074.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "76.   ./validation//images/0075.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "77.   ./validation//images/0076.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "78.   ./validation//images/0077.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "79.   ./validation//images/0078.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "80.   ./validation//images/0079.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "81.   ./validation//images/0080.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "82.   ./validation//images/0081.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "83.   ./validation//images/0082.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "84.   ./validation//images/0083.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "85.   ./validation//images/0084.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "86.   ./validation//images/0085.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "87.   ./validation//images/0086.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "88.   ./validation//images/0087.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "89.   ./validation//images/0088.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "90.   ./validation//images/0089.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "91.   ./validation//images/0090.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "92.   ./validation//images/0091.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "93.   ./validation//images/0092.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "94.   ./validation//images/0093.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "95.   ./validation//images/0094.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "96.   ./validation//images/0095.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "97.   ./validation//images/0096.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "98.   ./validation//images/0097.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "99.   ./validation//images/0098.png\n",
      "['x0: ', 49.78049087524414]\n",
      "['y0: ', 65.99351501464844]\n",
      "100.   ./validation//images/0099.png\n"
     ]
    }
   ],
   "source": [
    "# Painting circles on images from validation loader and placing them in directory \"./validation/draw/\". \n",
    "# Notice that if the painted circle is seen only partly, it means that it is not inside the \n",
    "# [0,1]x[0,1] \"box\", which is the domain considered.  This means that your model has siginificant error\n",
    "\n",
    "\n",
    "paint_loader_circles(model, validation_loader, './validation/draw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
